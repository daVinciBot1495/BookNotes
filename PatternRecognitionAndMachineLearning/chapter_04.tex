\documentclass[12pt]{article}

\usepackage{amsmath}

\newcommand{\x}{\mathbf{x}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\xA}{\mathbf{x}_A}
\newcommand{\xB}{\mathbf{x}_B}
\newcommand{\yOfX}{y\left(\x\right)}
\newcommand{\yOfXA}{y\left(\xA\right)}
\newcommand{\yOfXB}{y\left(\xB\right)}
\newcommand{\yOfXBot}{y\left(\x_\bot\right)}

\begin{document}
\section{Linear Models for Classification}
\subsection{Discriminant Functions}
\subsubsection{Two classes}
The simplest representation of a linear discriminant function is obtained by
taking a linear function of the input vector so that
%
\begin{equation*}
  \yOfX = \w^\top\x + w_0
\end{equation*}
%
where $\w$ is called the weight vector, and $w_0$ is a bias. If $\yOfX \geq 0$
then $\x$ is assigned to class $C_1$. Otherwise, it's assigned to class $C_2$.
Hence, the decision boundary is defined by the relation $\yOfX = 0$. Now,
consider two points $\xA$ and $\xB$ that both lie on the decision boundary
(i.e., $\yOfXA = \yOfXB = 0$)
%
\begin{align*}
  \yOfXA &= \yOfXB \\
  \w^\top\xA + w_0 &= \w^\top\xB + w_0 \\
  \w^\top\xA - \w^\top\xB &= 0 \\
  \w^\top\left(\xA - \xB\right) &= 0
\end{align*}
%
Note that the above equation is simply the equation of a hyperplane. This means
that the decision boundary is a planar decision surface whose orientation is
defined by $\w$.

Show that the distance in the direction of $\w$ from the hyperplane to the
origin is
%
\begin{equation*}
  \frac{-w_0}{\|\w\|}
\end{equation*}
%
To show this all we must do is find the length of the vector that results from
orthogonally projecting any point on the decision boundary onto $\w$.

Recall that the length of the orthogonal projection of a vector $\x$ onto a
vector $\w$ is
%
\begin{equation*}
  \frac{\w^\top\x}{\|\w\|}
\end{equation*}
%
Also, note that if a point $\x$ lies on the decision boundary then
%
\begin{align*}
  \yOfX &= 0 \\
  \w^\top\x + w_0 &= 0 \\
  \w^\top\x &= -w_0
\end{align*}
%
Therefore, when the point $\x$ is on the decision boundary the length of it's
orthogonal projection onto $\w$ is
%
\begin{equation*}
  \frac{\w^\top\x}{\|\w\|} = \frac{-w_0}{\|\w\|}  
\end{equation*}
%
Now, show that the distance of a point $\x$ from the hyperplane is
%
\begin{equation*}
  \frac{\yOfX}{\|\w\|}
\end{equation*}
%
To show this, let us decompose $\x$ into
%
\begin{equation*}
  \x = \x_\bot + r\frac{\w}{\|\w\|}
\end{equation*}
%
where $\x_\bot$ is the orthogonal projection of $\x$ onto the hyperplane and
$r\frac{\w}{\|\w\|}$ is a scalar multiple of the unit vector in the direction of
$\w$. Hence, r is the quantity of interest
%
\begin{align*}
  \x &= \x_\bot + r\frac{\w}{\|\w\|} \\
  \w^\top\x &= \w^\top\x_\bot + r\frac{\w^\top\w}{\|\w\|} \\
  \w^\top\x + w_0 &= \w^\top\x_\bot + w_0 + r\frac{\w^\top\w}{\|\w\|} \\
  \yOfX &= \yOfXBot + r\frac{\w^\top\w}{\|\w\|} \\
  \yOfX &= 0 + r\frac{\|\w\|^2}{\|\w\|} \\
  \yOfX &= r\|\w\| \\
  r &= \frac{\yOfX}{\|\w\|}
\end{align*}
%
Hence, the distance of a point $\x$ from the hyperplane is
%
\begin{equation*}
  \frac{\yOfX}{\|\w\|}
\end{equation*}
\end{document}
